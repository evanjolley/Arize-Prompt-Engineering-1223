{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3wjCnjv2IuE"
      },
      "outputs": [],
      "source": [
        "!pip3 install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "from openai import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "f7_KiNIj3HBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pull in data\n",
        "\n",
        "[Link](https://drive.google.com/file/d/1svDBMaYwv1uh_Z5yN9ZZl2n60Ep56fO0/view?usp=sharing) to download google_deepmind.txt\n",
        "\n",
        "[Link](https://drive.google.com/file/d/1wVHzI8N3oCyciC-tHERaUasU6OmFjxbC/view?usp=sharing) to download response_pairs.csv"
      ],
      "metadata": {
        "id": "cFjMwUgMizHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull in context data\n",
        "context = \"\"\n",
        "with open(\"google_deepmind.txt\", \"r\") as file:\n",
        "    context = file.read()\n",
        "\n",
        "# Pull in response pair data\n",
        "response_pairs = []\n",
        "with open(\"response_pairs.csv\", \"r\") as file:\n",
        "  response_pairs = list(csv.reader(file, delimiter=\",\"))\n",
        "\n",
        "questions = [row[0] for row in response_pairs]\n",
        "answers = [row[1] for row in response_pairs]"
      ],
      "metadata": {
        "id": "6I92x1wEYeHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define prompt templates"
      ],
      "metadata": {
        "id": "F1pEQKAiet4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template1 = \"Context: {context} Question: {question} Answer the question concisely based on information provided in the context. The answer should completely and accurately address the question asked using relevant context. Answer:\"\n",
        "\n",
        "prompt_template2 = \"{context} {question}\""
      ],
      "metadata": {
        "id": "NZu45-I1ewki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt function"
      ],
      "metadata": {
        "id": "4jNs7rJYhX-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt(prompt_template):\n",
        "  sampled_answers = []\n",
        "\n",
        "  for question in questions:\n",
        "    filled_prompt_template = prompt_template.format(context=context, question=question)\n",
        "    response_text = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": filled_prompt_template},\n",
        "        ]\n",
        "    ).json()\n",
        "\n",
        "    response_obj = json.loads(response_text)\n",
        "    content = response_obj[\"choices\"][0][\"message\"][\"content\"]\n",
        "    sampled_answers.append(content)\n",
        "\n",
        "  return sampled_answers"
      ],
      "metadata": {
        "id": "qyZNc8nChf2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define evaluation template"
      ],
      "metadata": {
        "id": "EK8__ahGigDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_template = \"You are given a correct answer and a sampled answer. You must determine whether the sampled answer is correct based on the correct answer. Here is the data:\\n[BEGIN DATA]\\n************\\n[question]: {question}\\n************\\n[correct answer]: {correct_answer}\\n************\\n[sampled_answer]: {sampled_answer}\\n[END DATA]\\n\\nYour response must be a single number, either 1 or 0, and should not contain any text or characters aside from that word. 1 means that the sampled answer answers the question as well as the given correct answer. 0 means it does not.\"\n"
      ],
      "metadata": {
        "id": "GWwCybTRijQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval function"
      ],
      "metadata": {
        "id": "mAQ1Nr6-haJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(sampled_answers):\n",
        "  total = 0\n",
        "\n",
        "  for i, sampled_answer in enumerate(sampled_answers):\n",
        "    filled_evaluation_template = evaluation_template.format(question=questions[i], correct_answer=answers[i], sampled_answer=sampled_answer)\n",
        "    response_text = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": filled_evaluation_template},\n",
        "        ]\n",
        "    ).json()\n",
        "\n",
        "    response_obj = json.loads(response_text)\n",
        "    content = response_obj[\"choices\"][0][\"message\"][\"content\"]\n",
        "    total += int(content)\n",
        "\n",
        "  score = total/len(sampled_answers)\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "2PJ1iPDniM5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt model using template #1"
      ],
      "metadata": {
        "id": "H9IcqZ95i45L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template1_answers = prompt(prompt_template1)\n"
      ],
      "metadata": {
        "id": "5DkeiIy9PiO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate template #1 answers"
      ],
      "metadata": {
        "id": "OIFYy961sE5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score1 = eval(template1_answers)"
      ],
      "metadata": {
        "id": "QgJ3BWzDsIbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(score1)"
      ],
      "metadata": {
        "id": "6cn6JcFpbviR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt model with template #2"
      ],
      "metadata": {
        "id": "QymXtqyqi_NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template2_answers = prompt(prompt_template2)"
      ],
      "metadata": {
        "id": "SU7YuuVCQMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate template #2 answers"
      ],
      "metadata": {
        "id": "qoxNo9nkIV09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score2 = eval(template2_answers)"
      ],
      "metadata": {
        "id": "f9rBG-fi8-uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(score2)"
      ],
      "metadata": {
        "id": "LF0feRYeO31V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}